\chapter{Cointegration}\label{chap:Coint}
In this chapter \cite{Analysis_of_integrated_and_cointegrated_time_series_with_R}, \cite{Cointegration_intro}, and \cite{co-Integration_and_error_correction} have been used. 
\pause
Concisely, cointegration is a concept which describes the correlation of two or more time series. Cointegration tests are used to identify if two or more non-stationary time series have a long-term equilibrium relation, meaning there exists a linear combination that is stationary.
\newline
\noindent In order to define cointegration, we first consider the concept of spurious regression.



\section{Spurious Regression}
In economic theory, trends are often encountered. If the trend is purely deterministic, then, in classic linear regression, it can either be removed before performing the regression or included in the model. It does not matter which of the solutions is used; the inference will be the same. However, this is not the case with non-stationary data. The error terms are often highly correlated in such case. The $t$ and $F$ statistics are ineffective when dealing with non-stationary data since the null hypothesis, which typically states that there are no relation, is rejected far too often for a given critical value. This regression is called spurious regression or nonsense regression and it is called such since it finds a relation between two phenomena that are not related. It is characterized by having a high $R^2$ value (close to $1$). This is initially due to the endogenous variable containing a stochastic trend and also that total variation is calculated as

%The $t$ and $F$ statistics can be biased or inflated, leading to the frequent rejection of the null hypothesis for a given critical value.

\begin{equation*}
    \sum_{t=1}^T(y_t-\Bar{y}),
\end{equation*}
where $\Bar{y}$ is the sample mean of the observations up until $y_t$ and $T$ is the length of the time series. This is an issue since it is incorrectly assumed that the series has a fixed mean in unison with the unadjusted goodness of fit measure given as
\begin{equation}
\label{goodness_of_fit_eq}
    R^2=1-\frac{\sum_{t=1}^T\hat{\varepsilon}^2_t}{\sum_{t=1}^T(y_t-\Bar{y_t})^2},
\end{equation}
where $\hat{\varepsilon_t}$ denotes the residual at time $t$, tends to 1 as the denominator grows, which is caused by the impact of extreme observations on either side of the sample mean $\Bar{y}$ and therefore spurious regression occurs. \bigskip \\

\noindent Granger and Newbold suggested as a rule of thumb (see\cite{Analysis_of_integrated_and_cointegrated_time_series_with_R} or \cite{Spurious_Regressions_in_Econonmetrics_1974}) tha caution should be taken if the goodness of fit measure \ref{goodness_of_fit_eq} is larger than the Durbin-Watson statistic, which is given as
\begin{equation*}
    DW=\frac{\sum_{t=2}^T(\hat{\varepsilon}_t-\hat{\varepsilon}_{t-1})^2}{\sum_{t=1}^T\hat{\varepsilon}_t}.
\end{equation*}
In theory, the spurious regression problem could be prevented by taking the difference. However, this creates two new problems. The first is the fact that differencing can reduce the positive residual autocorrelation, which could lead to false inferences about the coefficients. The second problem is that economic theories are often expressed in levels as they focus on the long-run relationships between variables. Hence, being forced to use regression methods, with differenced data could be a problem when testing the validity of economic theories. Other ways of transforming the non-stationary data to stationary have been shown to be successful, e.g. a logarithmic transformation.
The project at hand proceeds with another approach when dealing with trending variables, namely cointegration, with its definition stated below.

\begin{defi}{Cointegrated of order (d,b)}
    The components of the vector $\b{x}_t$ is said to be co-integrated of order $d$, $b$ if:
    \begin{itemize}
        \item All components of $\b{x}_t$ are $I(d)$.
        \item There exists a non-zero vector $\b{\beta}$ such that $z_t=\b{\beta}^\top \b{x}_t\sim I(d-b)$, $b>0$.
    \end{itemize}
    When the above conditions are fulfilled, we denote $\b{x}_t\sim CI(d,b)$ and $\b{\beta}$ is called the cointegration vector.
    \label{Def:Co-integrated of order d,b}
\end{defi}
 
\noindent With this definition, it is possible to detect stable long-run relationships between the non-stationary time series. When two time series are cointegrated, it is possible to find a linear combination of all entries in $\b{x}_t$ such that $\b{z}_t$ is a stationary series. Therefore, the combination of two individual non-stationary time series is stationary.
 

%Definition from \cite{co-Integration_and_error_correction}


\noindent An example of two time series that have a high goodness-of-fit measure but are not cointegrated is given below. 
\begin{ekse}{Spurious regression}
This example is taken from \cite{Spurious_example}.\\
    Let us consider the two non-stationary time series:
    \begin{enumerate}
        \item Viewership of "The Big Bang Theory"
        \item Google searches for "How to make a baby"
    \end{enumerate}
  The two time series give an $R^2$ value of $0.965$, indicating an excellent goodness-of-fit. This high value suggests that the model captures a relationship between the variables, making it an acceptable representation of the data. In reality, this is of course nonsense, meaning it is spurious regression.
\end{ekse}
\noindent This example clearly shows the need for Definition \ref{Def:Co-integrated of order d,b}, since the $R^2$ value is unreliable in situations like these.
\begin{comment}
\begin{defi}{Error Correction}
    A vector time series $x_t$ can be expressed using error correction if it can be written as:
    \begin{align*}
        A(B)(1-B)x_t=-\gamma z_{t_{t-1}}+u_t
    \end{align*}
    where $u_t$ is a stationary multivariate disturbance, $A(0)=I$, $A(1)$ has all elements finite, $z_\tau=\alpha^Tx_\tau$ and $\gamma\neq0$.
\end{defi}
Definition from \cite{co-Integration_and_error_correction}
The error correction model is a tool used when 
\end{comment}

\section{System of Cointegrated Variables}
This section covers the topic of multivariate cointegration. The $p$-dimensional vector autoregressive model VAR$(p)$, $\b{x}_t$, is given as
\begin{equation}
    \boldsymbol{x}_t=\Pi_1 \b{x}_{t-1} + \dots + \Pi_p \b{x}_{t-p} + \b{\mu} + \Phi \boldsymbol{d}_t +\boldsymbol{\varepsilon}_t \phantomsection \:\:\: \text{ for } \:\:\: t=1,\dots,T,
    \label{eq:p-dimensional_Var_model}
\end{equation}
where $\boldsymbol{x}_t \in \R^{k\times 1}$ is a vector of the time series at time $t$. The matrices $\Pi_i\in \R^{k\times k}$ are the coefficients of the lagged endogenous variables, $\boldsymbol{\mu} \in \R^{k\times 1}$ is a vector of constants, and $\boldsymbol{d}_t \in \R^{k\times 1}$ is the deterministic term, with $\Phi \in \R^{k\times k}$ being the corresponding coefficient matrix. The error term is assumed i.i.d $\b{\varepsilon}_t \sim N(\b{0},\Sigma)$. The vector error correction model (VECM) for $\b{x}_t$ can be derived from \eqref{eq:p-dimensional_Var_model} and is given below
% \begin{equation}
%     \Delta \boldsymbol{x}_t=\Gamma_1 \Delta \b{x}_{t-1} + \dots + \Gamma_{t-p} \Delta \b{x}_{t-p+1} + \Pi \b{x}_{t-p} + \b{\mu} + \boldsymbol{\Phi} \b{D}_t + \boldsymbol{\varepsilon}_t,
%     \label{eq:VECM_1}
% \end{equation}
\begin{equation}
    \Delta \boldsymbol{x}_t = \Pi \boldsymbol{x}_{t-p} + \sum_{i=1}^{p-1}\Gamma_i\Delta \boldsymbol{x}_{t-i} + \b{\mu} + \Phi \b{d}_t + \boldsymbol{\varepsilon}_t,
    \label{eq:VECM_1}
\end{equation}
where $\Pi=-(I-\Pi_1-\dots-\Pi_p)$ and 
$\Gamma_i=-(I-\Pi_{1}-\dots-\Pi_i) \text{ for } i=1,\dots,p-1$, which contains the cumulative long-run impacts.\\
Since it is assumed that each component in $\boldsymbol{x}_t$ is at most $I(1)$, the left-hand side is stationary. For the right-hand side to be stationary, the error correction term $\Pi\boldsymbol{x}_{t-p}$ must therefore also be stationary.
We examine the three possibilities for $\Pi$ below.
\begin{enumerate}
    \item If $\text{rank}(\Pi)=k$, then $\Pi$ has full rank, this means that all elements of $\b{x}_t$ are stationary, i.e., $I(0)$.
    \item If $\text{rank}(\Pi)=0$, $\Pi$ is the null matrix, therefore no cointegration is present, and hence the VECM \eqref{eq:VECM_1} is reduced to a standard VAR model.
    \item If $0<\text{rank}(\Pi)=r<k$, then there exist $r$ cointegration vectors, which means the system has $r$ linear stationary combinations of the non-stationary variables in $\b{x}_t$.
\end{enumerate}
In the first two cases, there is no cointegration and therefore no use for the VECM. The third and last case is interesting since cointegration is present and therefore $r$ cointegrating relations exist with $0<r<k$, meaning $\Pi$ is neither the null matrix nor is it full rank. Therefore there exist two matrices $\alpha$ and $\beta$ both in $\R^{k\times r}$, such that $\Pi=\alpha \beta^\top$. Here, $\alpha$ is the adjustment coefficient matrix, which determines the speed of the adjustments towards a long-run equilibrium. The matrix $\beta$ contains the cointegration vectors, which are the $r$ linearly independent rows, and the linear combination $\beta^\top \b{x}_t$ is stationary. This system can be modeled by the VECM \eqref{eq:VECM_1}, and using $\Pi=\alpha\beta^\top$ it can be written as

\begin{equation}
\Delta \boldsymbol{x}_t = \alpha\beta^\top \boldsymbol{x}_{t-p} + \sum_{i=1}^{p-1}\Gamma_i\Delta \boldsymbol{x}_{t-i} + \b{\mu} + \Phi \b{d}_t + \boldsymbol{\varepsilon}_t.
    \label{eq:VECM_2}
\end{equation}

\section{Granger Representation}
The Granger Representation Theorem links cointegration and the VECM, as it states that if a set of variables are cointegrated, then a VECM exists.\\
%
\noindent 
Before introducing the Granger Representation Theorem, the characteristic matrix polynomial $\Pi(z)$ is given as
\begin{equation*}
    \Pi(z)=(1-z)I_{k\times k}-\Pi z-(1-z)\sum_{i=1}^{p-1}\Gamma_i z^i,
\end{equation*}
\noindent with $\text{det}(\Pi(z))$ not exceeding $kp$ degrees. Let $\rho^{-1}_i$ be the roots of $\text{det}(\Pi(z)) = 0$. Then $\text{det}(\Pi(z)) = \Pi^{kp}_{i=1}(1-z \rho_i)$ and the inverse matrix is given by 
\begin{equation*}
    C(z)=\Pi^{-1}(z)=\frac{\text{adj}(\Pi(z))}{\text{det}(\Pi(z))}, \phantom{asd} z\neq \rho^{-1}_i.
\end{equation*}

\begin{thm}{\hfill}
    \noindent If $\rho_i<1$, the coefficients of $\Pi^{-1}(z)=C(z)=\sum_{i=0}^\infty C_i\b{z}^i$ will be exponentially decreasing. Let $\boldsymbol{z}_t=\sum_{i=0}^\infty C_i\b{\varepsilon}_{t-i}$ and $\boldsymbol{\mu}_t=\sum_{i=0}^\infty C_i\Phi \boldsymbol{d}_{t-i}$, then the initial values of $\boldsymbol{x}_t$ can be given a distribution such that $\boldsymbol{x}_t - \mathbb{E}[\b{x}_t]$ is stationary. The moving average representation of $\b{x}_t$ is 
    \begin{equation}
        \boldsymbol{x}_t=\sum_{i=0}^\infty C_i(\b{\varepsilon}_{t-i}+\Phi \boldsymbol{d}_{t-i})=\b{z}_t + \b{\mu}_t.
        \label{eq:moving_average_representation}
    \end{equation}
\end{thm}
\noindent The proof of this theorem has been omitted, but can be found on page 14 in \cite{Likelihood-based-inference-book}.\\
\noindent The process \eqref{eq:moving_average_representation} is a linear process and is used to define integration and cointegration.
\begin{defi}{\hfill}
    We say that the process $\b{x}_t$ is integrated of order $1$, $I(1)$, if $\Delta \boldsymbol{x}_t$ is a linear process, with 
    \begin{equation*}
        C(1) = \sum^\infty_{i=0} C_i \neq 0.
    \end{equation*}
    If there is a vector $\b{\beta}_i \neq \b{0}$ such that $\b{\beta}_i^\top \b{x}_t$ is stationary, then $\boldsymbol{x}_t$ is cointegrated with the cointegration vector $\b{\beta}_i$. The number of linearly independent cointegration vectors is the cointegrating rank.
\end{defi}

\noindent If the characteristic polynomial $\Pi(z)$ has a unit root, then $\Pi(1)=-\Pi$ is singular and has rank $r$, which gives us a non-stationary process. 

% Then the matrix $\b{\beta'} \in \R^{r \times p }$ matrix denotes the r linearly independent rows of $\Pi$ and let this gives us a non stationary process.
% Then the $\b{\alpha}\in \R^{r \times p}$ matrix contain the coefficients that express each row of $-\Pi$ as a combination of vectors $\b{\beta'}$ this means that $\Pi=\b{ \alpha\beta'}$ from this and  \eqref{eq:VECM_1} we can write
% \begin{equation*}
% \Delta \boldsymbol{x}_t = \alpha\beta^\top \boldsymbol{x}_{t-p} + \sum_{i=1}^{p-1}\Gamma_i\Delta \boldsymbol{x}_{t-i} + \b{\mu} + \boldsymbol{\Phi} \b{d}_t + \boldsymbol{\varepsilon}_t.
% \end{equation*}
\noindent
We define $\Gamma=I-\sum_{i=1}^{p-1}\Gamma_i$, which is used to construct the following condition.

\begin{condition}\phantom{}\\
    This condition assumes that $\text{det}(\Pi(z))=0$ which implies that $|z|>1$ or $z=1$ and also that $\text{det}(\alpha^\top_{\perp}\Gamma \beta_{\perp})\neq0$, with $\alpha_{\perp}$ being the orthogonal complement of $\alpha$, this is the same as saying $\text{det}(\Pi(z))=0$ has rank $k-r$.
    \label{condition_granger}
\end{condition}

% \begin{enumerate} \label{condition_granger}
%     \item condition says we assume that $det(\Pi(z))=0$ which implies that $|z|>1$ or $z=1$ and assmue also that $det(\b{\alpha}'_{\perp}\Gamma\b{\beta}_{\perp})\neq0$ this is the same as saying $det(\Pi(z))=0$ have rank $p-r$
% \end{enumerate}
\noindent When the condition above is fulfilled then there are no solutions with seasonal or explosive roots and neither are there no solutions of order two or higher. The Granger Representation Theorem will now be formulated.
\begin{thm}{The Granger Representation Theorem}
    If $\Pi(z)$ has a unit root and Condition \ref{condition_granger} is satisfied, then
    \begin{equation}\label{Granger_representation_eq_1}
        (1-z)\Pi^{-1}(z)=C(z)=\sum^{\infty}_{i=0}C_iz^i=C(1)+(1-z)C^\ast(z)
    \end{equation}
    is convergent for $|z|\leq1+\delta
    $ for some $\delta>0$,
    \begin{equation*}
C^{\ast}(z)=\frac{C(z)-C(1)}{1-z}\text{ for }z\neq 1\;\;\;\;\;\;\;\text{and}\;\;\;\;\;\;\;C=C(1)=\beta_\perp(\alpha^\top_{\perp}\Gamma\beta_{\perp})^{-1}\alpha^\top_{\perp}.
    \end{equation*}
The process $\b{x}_t$ has the moving average representation
\begin{equation}\label{granger_representation_eq_3}
    \b{x}_t=C\sum_{i=1}^{t}(\b{\varepsilon}_{i}+\Phi d_i)+\sum_{i=0}^{\infty}C_{i}^\ast(\b{\varepsilon}_{t-i}+\b{\Phi} \b{d}_{t-i})+\b{a}
\end{equation}
where $\b{a}$ depends on the initial values, such that $\beta^\top\b{a}=0$. It follows that $\b{x}_t$ is a cointegrated $I(1)$ process with $r$ cointegration vectors $\beta$ and $k-r$ shared stochastic trends $\alpha^\top_{\perp}\sum_{i=1}^{t}\b{\varepsilon}_i$.
\end{thm}
\begin{bema}
    The result from \eqref{Granger_representation_eq_1} rests on the observation that the singularity of $\Pi(z)$ for $z=1$ implies that $\Pi(z)^{-1}$ has a pole at $z=1$. Condition \ref{condition_granger} ensures that the pole is of order one.
\end{bema}
\noindent The entire proof can be found in \cite{co-Integration_and_error_correction}. We will instead focus on proving the MA representation \eqref{granger_representation_eq_3}.
\begin{bevis}
    To prove \eqref{granger_representation_eq_3} we multiply 
    \begin{align*}
        \Pi(L)\b{x}_t=\b{\varepsilon}_t + \b{\Phi} \b{d}_t
    \end{align*}
    by
    \begin{align*}
        (1-L)\Pi^{-1}(L)=C(L)=C(1)+(1-L)C^\ast(L)
    \end{align*}
    this gives us
    \begin{align*}
        \Delta \b{x}_t= (1-L)\Pi^{-1}(L)\Pi(L)\b{x}_t=C(1)(\b{\varepsilon}_t+\b{\Phi} \b{d}_t)+(1-L) C^\ast(L)(\b{\varepsilon}_t+\b{\Phi} \b{d}_t).
    \end{align*}
    % We define the stationary process $z_t=C^\ast (L)\varepsilon_t = \sum_{i=0}^\infty C_i^\ast \b{\varepsilon}_{t-i}$ and the deterministic function $\b{\mu}_t=C^\ast(L)\Phi d_t= \sum_{i=0}^\infty C_i^\ast \Phi d_{t-i} $. Then

\noindent We define the stationary process $\b{z}_t=C^\ast (L)\b{\varepsilon}_t$ and the deterministic function $\b{\mu}_t=C^\ast(L)\b{\Phi} \b{d}_t$ then we are able to rewrite $\Delta \b{x}_t$ as
    \begin{align*}
        \Delta \b{x}_t&=C(\b{\varepsilon}_t+\b{\Phi} \b{d}_t)+(1-L)(\b{z}_t+\b{\mu}_t)=C(\b{\varepsilon}_t+\b{\Phi} \b{d}_t)+(\b{z}_t-\b{z}_{t-1}+\b{\mu}_t-\b{\mu}_{t-1}).
        \end{align*}
        Next $\b{x}_t$ will be derived iteratively computing $\b{x}_t$ and its lags:
        \begin{align*}
    \b{x}_t&=\Delta \b{x}_t+\b{x}_{t-1}=C(1)(\b{\varepsilon}_t+\b{\Phi} \b{d}_t)+(\b{z}_t-\b{z}_{t-1}+\b{\mu}_t-\b{\mu}_{t-1})+\b{x}_{t-1} \\
     \b{x}_{t-1}&=\Delta \b{x}_{t-1}+\b{x}_{t-2}=C(1)(\b{\varepsilon}_{t-1}+\b{\Phi} \b{d}_{t-1})+(\b{z}_{t-1}-\b{z}_{t-2}+\b{\mu}_{t-1}-\b{\mu}_{t-2})+\b{x}_{t-2}\\
     &\vdots\\
     \b{x}_2&=\Delta \b{x}_2+\b{x}_{1}=C(1)(\b{\varepsilon}_2+\b{\Phi} \b{d}_2)+(\b{z}_2-\b{z}_{1}+\b{\mu}_2-\b{\mu}_{1})+\b{x}_{1} \\
     \b{x}_1&=\Delta \b{x}_1+\b{x}_{0}=C(1)(\b{\varepsilon}_1+\b{\Phi} \b{d}_1)+(\b{z}_1-\b{z}_{0}+\b{\mu}_1-\b{\mu}_{0})+\b{x}_{0}
    \end{align*}
    Combining the equations above a sum over $C(1)(\b{\varepsilon}_i+\b{\Phi} \b{d}_i)$ and the telescoping sum $\b{z}_i-\b{z}_{i-1}+\b{\mu}_i-\b{\mu}_{i-1}$ is achieved. Furthermore we define $\b{a}=\b{x}_0-\b{z}_0-\b{\mu}_0$, then $\b{x}_t$ can be written as
\begin{align}
    \b{x}_t=C\sum^{t}_{i=1}(\b{\varepsilon}_i+\b{\Phi} \b{d}_i)+\b{z}_t+\b{\mu}_t+\b{a}.
\end{align}
Here \eqref{eq:moving_average_representation} can be used to write the infinite MA representation of $\b{z}_t+\b{\mu}_t$:
%Using the definition for $\b{z}_t$ and $\b{\mu}_t$ the infinite MA representation can be inserted
\begin{align}
    \b{x}_t=C\sum^{t}_{i=1}(\b{\varepsilon}_i+\b{\Phi} \b{d}_i)+\sum_{i=0}^{\infty}C^\ast_i(\b{\varepsilon}_{t-i}+\b{\Phi} \b{d}_{t-i})+\b{a}
\end{align} 
We choose the distribution of $\b{x}_0$ such that $\b{\beta}^{\top}\b{x}_0=\b{\beta}^\top(\b{z}_0+\b{\mu}_0)$, and here $\b{\beta}^\top \b{a}=0$. It is seen that $\b{x}_t$ is $I(1)$, $\b{\beta}^\top \b{x}_t=\b{\beta}^{\top}\b{z}_t+\b{\beta}^\top \b{\mu}_t$, such that $\b{\beta}^\top \b{x}_t$ is stationary around its mean $\mathbb{E}[\b{\beta}^{\top} \b{x}_t]=\b{\beta}^{\top} \b{\mu}_t$, and that $\Delta \b{x}_t$ is stationary around its mean $\mathbb{E}(\Delta \b{x}_t) =C\b{\Phi} \b{d}_t +\Delta\b{\mu}_t$. This concludes the proof.

\end{bevis}





\section{Classification of the Various Hypotheses}\label{subsec:classification_of_various_hypot}
In this section \cite{MAXIMUM_LIKELIHOOD_ESTIMATION_AND_INFERENCE_ON_COINTEGRATION_—_WITH_APPLICATIONS_TO_THE_DEMAND_FOR_MONEY} has been used.\\
%Kilde:  https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1468-0084.1990.mp52002003.x
Consider the unrestricted hypothesis $H_1$ given in \eqref{eq:p-dimensional_Var_model}. The various hypotheses that are given in Table \ref{tab:Various_Hypotheses} are related to the existence of cointegrating relations combined with linear restrictions.
\begin{table}[H]
    \centering
    \begin{tabular}{l}
     $H_2:$ $\Pi=\alpha\beta^\top$.      \\
     $H_3:$ $\Pi=\alpha\varphi^\top H^\top$.      \\
     $H_4:$ $\Pi=A \psi \beta^\top$.      \\
     $H_5:$ $\Pi=A \psi \varphi^\top H^\top$.       \\
    \end{tabular}
    \caption{Hypotheses}
    \label{tab:Various_Hypotheses}
\end{table}
\noindent As previously mentioned, if the process is non-stationary, there is an absence of a linear trend and the augmented hypotheses are used, $H^\ast_j$ for $j=2,\dots,5$, which are augmented by $\b{\mu}=\alpha\beta^\top_0$, this is equivalent to $\alpha^\top_\perp \b{\mu}=\b{0}$. The matrices $A\in \R^{k\times m}$ and $H\in \R^{k\times s}$ are known and define a linear restrictions on the parameters $\alpha$ and $\beta$. The restrictions reduce the parameters to $\varphi \in \R^{s \times r}$ and $\psi \in \R^{m \times r}$, where $r \leq s \leq k$ and $r \leq m \leq k$.
\pause

\noindent The main hypothesis of interest in this project is $H_2$. It should be noted that for each value of $r$, there is a corresponding hypothesis $H_j(r)$ which states there are $r$ or fewer cointegrating relations. 
 \newline
 The process makes it possible to conduct inferences about the number of cointegrating relations through hypothesis testing. It can be tested if $H_2(r)$ lies in $H_1$ or by testing if an increase of one in the number of cointegration vectors improves the model, hence testing if $H_2(r)$ lies in $H_2(r+1)$. The last approach forms a nested sequence of the models $H(r)$
%The models $H(r)$ form nested sequence
\begin{equation*}
    H(0) \subset \dots \subset H(r) \subset \dots \subset H(k),
\end{equation*}
where the processes $H(r)$ contains the processes with cointegration equal to or less than $r$. The formulation above allows for the derivation of the likelihood ratio test for hypothesis $H(r)$ in $H(k)$. These tests can then be applied to check if prior knowledge of the number of cointegrating relations is consistent with the data or to construct an estimator of the cointegrating rank.



